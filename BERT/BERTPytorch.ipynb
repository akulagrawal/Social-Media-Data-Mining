{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAI7h5migy-",
        "colab_type": "code",
        "outputId": "7fe631c4-112c-408c-e0b1-d4730c8738fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNg9wvidj2J2",
        "colab_type": "code",
        "outputId": "bbefca0b-bc4d-47a0-c9f3-40234b1ef531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "path = '/content/drive/My Drive/CS529'\n",
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.4)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.17 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.17)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.17->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.17->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WqKz3hsjwnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghHnHLPFi6ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_file(filepath):\n",
        "\tlang = []\n",
        "\tdata = []\n",
        "\n",
        "\twith open(filepath, 'r') as file_object:\n",
        "\t\tline = file_object.readline()\n",
        "\t\tf = open(path + \"/my_test.txt\",\"w\")\n",
        "\t\twhile line:\n",
        "\t\t    a =  re.findall(r\"[\\w']+\", line)\n",
        "\t\t    # a = line.strip().split(' ')\n",
        "\t\t    for i in range(len(a)):\n",
        "\t\t       if i % 2 != 1:\n",
        "\t\t         data.append(a[i])\n",
        "\t\t       if i % 2 != 0:\n",
        "\t\t         lang.append(a[i])\n",
        "\t\t    res = [a[j] for j in range(len(a)) if j % 2 != 1] \n",
        "\t\t    # print(res)\n",
        "\t\t    listToStr = ' '.join([str(elem) for elem in res]) \n",
        "\t\t    f.write(listToStr + \"\\n\") \n",
        "\n",
        "\t\t    line = file_object.readline()\n",
        "\n",
        "\t\tf.close()\n",
        "\treturn data, lang"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB-0M5BJjhx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fun():\n",
        "  data, lang = parse_file(path + \"/test.txt\")\n",
        "  with open(path + '/my_test.txt') as f:\n",
        "    lines = f.readlines()\n",
        "  idx = 0\n",
        "  col = []\n",
        "  for i in range(768):\n",
        "    col.append(str(i))\n",
        "  df = pd.DataFrame(columns = col)\n",
        "  skipped = []\n",
        "  ## Load pre-trained model tokenizer (vocabulary)\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  for i in tqdm(range(len(lines))):\n",
        "    text = lines[i]\n",
        "    #print(text)\n",
        "    \n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    #tokenized_text = marked_text.split()\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    # Load pre-trained model (weights)\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "\n",
        "      # Predict hidden states features for each layer\n",
        "      with torch.no_grad():\n",
        "          encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "      #print (\"Number of layers:\", len(encoded_layers))\n",
        "      layer_i = 0\n",
        "\n",
        "      #print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "      batch_i = 0\n",
        "\n",
        "      #print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "      token_i = 0\n",
        "\n",
        "      #print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "\n",
        "      # Holds the list of 12 layer embeddings for each token\n",
        "      # Will have the shape: [# tokens, # layers, # features]\n",
        "      token_embeddings = [] \n",
        "\n",
        "      # For each token in the sentence...\n",
        "      for token_i in range(len(tokenized_text)):\n",
        "        \n",
        "        # Holds 12 layers of hidden states for each token \n",
        "        hidden_layers = [] \n",
        "        \n",
        "        # For each of the 12 layers...\n",
        "        for layer_i in range(len(encoded_layers)):\n",
        "          \n",
        "          # Lookup the vector for `token_i` in `layer_i`\n",
        "          vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "          \n",
        "          hidden_layers.append(vec)\n",
        "          \n",
        "        token_embeddings.append(hidden_layers)\n",
        "\n",
        "      # Sanity check the dimensions:\n",
        "      #print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
        "      #print (\"Number of layers per token:\", len(token_embeddings[0]))\n",
        "\n",
        "      #concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
        "\n",
        "      summed_last_1_layers = [torch.sum(torch.stack(layer)[-1:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
        "      \n",
        "      for token in range(1,len(tokenized_text)-1):\n",
        "        if tokenized_text[token][0] == '#':\n",
        "          continue\n",
        "        df.loc[len(df)] = summed_last_1_layers[token].numpy()\n",
        "      idx = idx+1\n",
        "      # Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"river bank\"\n",
        "      #different_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[19].reshape(1,-1))[0][0]\n",
        "    except:\n",
        "      skipped.append(idx)\n",
        "      idx=idx+1\n",
        "\n",
        "    # Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"bank vault\" \n",
        "    #same_bank = cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[6].reshape(1,-1))[0][0]\n",
        "  df.to_csv(path + \"/test_data.csv\", index=False)\n",
        "  return skipped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6irvpcmkff-",
        "colab_type": "code",
        "outputId": "5af4da6e-aab5-49e1-feef-b737e1bae035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "skipped = fun()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2599653.91B/s]\n",
            "  0%|          | 0/2390 [00:00<?, ?it/s]\n",
            "  0%|          | 0/407873900 [00:00<?, ?B/s]\u001b[A\n",
            "  0%|          | 261120/407873900 [00:00<02:53, 2350936.45B/s]\u001b[A\n",
            "  1%|          | 2525184/407873900 [00:00<02:06, 3215379.39B/s]\u001b[A\n",
            "  2%|▏         | 8710144/407873900 [00:00<01:28, 4490956.02B/s]\u001b[A\n",
            "  4%|▍         | 15578112/407873900 [00:00<01:02, 6240756.70B/s]\u001b[A\n",
            "  6%|▌         | 22800384/407873900 [00:00<00:44, 8596862.18B/s]\u001b[A\n",
            "  7%|▋         | 30239744/407873900 [00:00<00:32, 11701686.83B/s]\u001b[A\n",
            "  9%|▉         | 37851136/407873900 [00:00<00:23, 15683298.09B/s]\u001b[A\n",
            " 11%|█         | 44022784/407873900 [00:00<00:18, 20157652.14B/s]\u001b[A\n",
            " 13%|█▎        | 51319808/407873900 [00:00<00:13, 25748135.97B/s]\u001b[A\n",
            " 14%|█▍        | 58843136/407873900 [00:01<00:10, 32077897.68B/s]\u001b[A\n",
            " 16%|█▋        | 66301952/407873900 [00:01<00:08, 38693586.49B/s]\u001b[A\n",
            " 18%|█▊        | 73671680/407873900 [00:01<00:07, 45122513.44B/s]\u001b[A\n",
            " 20%|█▉        | 81061888/407873900 [00:01<00:06, 50980692.63B/s]\u001b[A\n",
            " 22%|██▏       | 88776704/407873900 [00:01<00:05, 56755894.00B/s]\u001b[A\n",
            " 24%|██▎       | 96109568/407873900 [00:01<00:05, 60606206.36B/s]\u001b[A\n",
            " 25%|██▌       | 103869440/407873900 [00:01<00:04, 64866922.66B/s]\u001b[A\n",
            " 27%|██▋       | 111302656/407873900 [00:01<00:04, 67238785.57B/s]\u001b[A\n",
            " 29%|██▉       | 119015424/407873900 [00:01<00:04, 69927836.16B/s]\u001b[A\n",
            " 31%|███       | 126515200/407873900 [00:01<00:04, 61626761.35B/s]\u001b[A\n",
            " 33%|███▎      | 133213184/407873900 [00:02<00:05, 51134328.86B/s]\u001b[A\n",
            " 34%|███▍      | 139754496/407873900 [00:02<00:04, 54717537.98B/s]\u001b[A\n",
            " 36%|███▌      | 146270208/407873900 [00:02<00:04, 57480108.61B/s]\u001b[A\n",
            " 38%|███▊      | 153245696/407873900 [00:02<00:04, 60653294.73B/s]\u001b[A\n",
            " 39%|███▉      | 160353280/407873900 [00:02<00:03, 63443985.24B/s]\u001b[A\n",
            " 41%|████      | 166968320/407873900 [00:02<00:03, 63667498.69B/s]\u001b[A\n",
            " 43%|████▎     | 173524992/407873900 [00:02<00:03, 63116389.84B/s]\u001b[A\n",
            " 44%|████▍     | 180156416/407873900 [00:02<00:03, 64042018.60B/s]\u001b[A\n",
            " 46%|████▌     | 187439104/407873900 [00:02<00:03, 66294102.06B/s]\u001b[A\n",
            " 48%|████▊     | 194156544/407873900 [00:03<00:03, 66400493.52B/s]\u001b[A\n",
            " 49%|████▉     | 201805824/407873900 [00:03<00:02, 69136828.37B/s]\u001b[A\n",
            " 51%|█████▏    | 209552384/407873900 [00:03<00:02, 71440878.75B/s]\u001b[A\n",
            " 53%|█████▎    | 217099264/407873900 [00:03<00:02, 72602895.49B/s]\u001b[A\n",
            " 55%|█████▌    | 224413696/407873900 [00:03<00:02, 72496108.53B/s]\u001b[A\n",
            " 57%|█████▋    | 231701504/407873900 [00:03<00:02, 70035145.94B/s]\u001b[A\n",
            " 59%|█████▊    | 238749696/407873900 [00:03<00:02, 67330299.63B/s]\u001b[A\n",
            " 60%|██████    | 245944320/407873900 [00:03<00:02, 68650891.86B/s]\u001b[A\n",
            " 62%|██████▏   | 253860864/407873900 [00:03<00:02, 71499420.67B/s]\u001b[A\n",
            " 64%|██████▍   | 261611520/407873900 [00:03<00:01, 73200592.31B/s]\u001b[A\n",
            " 66%|██████▌   | 268985344/407873900 [00:04<00:01, 71539168.90B/s]\u001b[A\n",
            " 68%|██████▊   | 276600832/407873900 [00:04<00:01, 72862021.02B/s]\u001b[A\n",
            " 70%|██████▉   | 284214272/407873900 [00:04<00:01, 73811631.45B/s]\u001b[A\n",
            " 71%|███████▏  | 291624960/407873900 [00:04<00:01, 73252574.27B/s]\u001b[A\n",
            " 73%|███████▎  | 298972160/407873900 [00:04<00:01, 63095904.93B/s]\u001b[A\n",
            " 75%|███████▍  | 305545216/407873900 [00:04<00:01, 55253828.66B/s]\u001b[A\n",
            " 76%|███████▋  | 312005632/407873900 [00:04<00:01, 57759523.47B/s]\u001b[A\n",
            " 78%|███████▊  | 319240192/407873900 [00:04<00:01, 61476205.03B/s]\u001b[A\n",
            " 80%|████████  | 326790144/407873900 [00:05<00:01, 65100498.32B/s]\u001b[A\n",
            " 82%|████████▏ | 333542400/407873900 [00:05<00:01, 65408039.69B/s]\u001b[A\n",
            " 84%|████████▎ | 341230592/407873900 [00:05<00:00, 68473551.92B/s]\u001b[A\n",
            " 86%|████████▌ | 349280256/407873900 [00:05<00:00, 71684093.07B/s]\u001b[A\n",
            " 88%|████████▊ | 357251072/407873900 [00:05<00:00, 73913820.78B/s]\u001b[A\n",
            " 89%|████████▉ | 364902400/407873900 [00:05<00:00, 74672909.55B/s]\u001b[A\n",
            " 91%|█████████▏| 373044224/407873900 [00:05<00:00, 76575770.38B/s]\u001b[A\n",
            " 93%|█████████▎| 381102080/407873900 [00:05<00:00, 77734133.12B/s]\u001b[A\n",
            " 95%|█████████▌| 388931584/407873900 [00:05<00:00, 74192894.60B/s]\u001b[A\n",
            " 97%|█████████▋| 396812288/407873900 [00:05<00:00, 75518117.41B/s]\u001b[A\n",
            " 99%|█████████▉| 404715520/407873900 [00:06<00:00, 76537656.59B/s]\u001b[A\n",
            "100%|██████████| 2390/2390 [9:24:22<00:00, 15.00s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c2BLPKSklqg",
        "colab_type": "code",
        "outputId": "c90ad16e-cab7-4383-a7f7-e4975c62289b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "with open(path + '/my.txt') as f:\n",
        "    lines = f.readlines()\n",
        "data, lang = parse_file(path + \"/train.txt\")\n",
        "s=0\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "langtoken = []\n",
        "sum=0\n",
        "for i in tqdm(range(len(lines))):\n",
        "    text = lines[i]\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    words = marked_text.split()\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    #print(words)\n",
        "    #print(tokenized_text)\n",
        "    tokenidx = 1\n",
        "    for token in range(1,len(tokenized_text)-1):\n",
        "        if tokenized_text[token][0] == '#':\n",
        "          langtoken.append(0)\n",
        "          continue\n",
        "        sum=sum+1\n",
        "        langtoken.append(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
            " 14%|█▎        | 135/1000 [00:00<00:00, 1293.68it/s]\u001b[A\n",
            " 27%|██▋       | 272/1000 [00:00<00:00, 1315.62it/s]\u001b[A\n",
            " 41%|████      | 406/1000 [00:00<00:00, 1322.72it/s]\u001b[A\n",
            " 50%|████▉     | 497/1000 [00:00<00:00, 1152.45it/s]\u001b[A\n",
            " 62%|██████▏   | 616/1000 [00:00<00:00, 1153.30it/s]\u001b[A\n",
            " 74%|███████▍  | 743/1000 [00:00<00:00, 1088.59it/s]\u001b[A\n",
            " 87%|████████▋ | 870/1000 [00:00<00:00, 1136.53it/s]\u001b[A\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1192.12it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru2AZQwRLB-h",
        "colab_type": "code",
        "outputId": "8c200668-93c2-4c29-f586-3b22860a43d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(langtoken))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8iX0ZXlMhkU",
        "colab_type": "code",
        "outputId": "0d1065ef-5a1f-49d6-a98c-b5c66a541e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(skipped)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_jjO4GwKwwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs = pd.DataFrame(skipped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWzuQmtAK3dZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs.to_csv(path+'/skipped.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSutMa_K_JM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(path + '/my.txt') as f:\n",
        "    lines = f.readlines()\n",
        "data, lang = parse_file(path + \"/train.txt\")\n",
        "skipped = pd.read_csv(path+'/skipped.csv')\n",
        "labels = []\n",
        "idx=0\n",
        "for i in range(len(lines)):\n",
        "  text = lines[i]\n",
        "  words = text.split()\n",
        "  if i in skipped['0'].tolist():\n",
        "    idx=idx+len(words)\n",
        "    continue\n",
        "  for token in words:\n",
        "    labels.append(lang[idx])\n",
        "    idx = idx+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Im9kNcWbNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfl = pd.DataFrame(labels)\n",
        "dfl.to_csv(path+'/labels_2.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m3PATL_PsVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, lang = parse_file(path + \"/test.txt\")\n",
        "dft = pd.DataFrame(lang)\n",
        "dft.to_csv(path + '/test_labels.csv',index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFEeWC2WP5C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}